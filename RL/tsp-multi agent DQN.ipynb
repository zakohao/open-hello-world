{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2e2d42",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from scipy.spatial import distance_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "import copy\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 定义设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定义经验回放缓冲区\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# 定义DQN网络\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 多智能体TSP环境\n",
    "class MultiAgentTSPEnv:\n",
    "    def __init__(self, cities, n_agents):\n",
    "        self.cities = np.array(cities)\n",
    "        self.n_cities = len(cities)\n",
    "        self.n_agents = n_agents\n",
    "        self.distances = distance_matrix(self.cities, self.cities)\n",
    "        \n",
    "        # 起点：城市0\n",
    "        self.start_city = 0\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        # 所有智能体都在起点\n",
    "        self.agent_positions = [self.start_city] * self.n_agents\n",
    "        \n",
    "        # 访问状态：起点已经被访问（因为所有智能体都在起点）\n",
    "        self.visited = np.zeros(self.n_cities, dtype=bool)\n",
    "        self.visited[self.start_city] = True\n",
    "        \n",
    "        # 记录每个智能体的路径和总距离\n",
    "        self.paths = [[self.start_city] for _ in range(self.n_agents)]\n",
    "        self.total_distances = np.zeros(self.n_agents)\n",
    "        \n",
    "        # 记录每个智能体是否完成\n",
    "        self.completed = [False] * self.n_agents\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # 状态包括：\n",
    "        # 1. 所有智能体的当前位置 (n_agents 维)\n",
    "        # 2. 城市访问状态 (n_cities 维)\n",
    "        # 3. 每个智能体的完成状态 (n_agents 维)\n",
    "        # 4. 每个智能体已访问的城市数 (n_agents 维)\n",
    "        \n",
    "        state = np.concatenate([\n",
    "            np.array(self.agent_positions) / (self.n_cities - 1),  # 归一化位置\n",
    "            self.visited.astype(float),\n",
    "            np.array(self.completed).astype(float),\n",
    "            np.array([len(set(path)) for path in self.paths]) / self.n_cities  # 归一化访问数\n",
    "        ])\n",
    "        return state\n",
    "    \n",
    "    def step(self, actions):\n",
    "        rewards = np.zeros(self.n_agents)\n",
    "        new_positions = self.agent_positions.copy()\n",
    "        \n",
    "        # 记录哪些城市被访问了（用于更新访问状态）\n",
    "        visited_this_step = np.zeros(self.n_cities, dtype=bool)\n",
    "        \n",
    "        # 处理每个智能体的动作\n",
    "        for i, action in enumerate(actions):\n",
    "            if self.completed[i]:\n",
    "                # 已完成任务的智能体不执行动作\n",
    "                continue\n",
    "                \n",
    "            if action < self.n_cities:  # 移动到新城市\n",
    "                target_city = action\n",
    "                \n",
    "                # 计算移动距离\n",
    "                dist = self.distances[self.agent_positions[i]][target_city]\n",
    "                self.total_distances[i] += dist\n",
    "                \n",
    "                # 更新位置\n",
    "                new_positions[i] = target_city\n",
    "                self.paths[i].append(target_city)\n",
    "                \n",
    "                # 如果访问了新城市\n",
    "                if not self.visited[target_city]:\n",
    "                    # 给予探索奖励\n",
    "                    rewards[i] += 5.0\n",
    "                    visited_this_step[target_city] = True\n",
    "            else:  # 停留在原地\n",
    "                # 轻微惩罚停留\n",
    "                rewards[i] -= 0.1\n",
    "        \n",
    "        # 更新访问状态\n",
    "        self.visited = np.logical_or(self.visited, visited_this_step)\n",
    "        \n",
    "        # 更新位置\n",
    "        self.agent_positions = new_positions\n",
    "        \n",
    "        # 检查智能体是否完成\n",
    "        for i in range(self.n_agents):\n",
    "            if not self.completed[i] and len(set(self.paths[i])) == self.n_cities:\n",
    "                self.completed[i] = True\n",
    "                # 完成奖励\n",
    "                rewards[i] += 20.0\n",
    "        \n",
    "        # 检查是否所有智能体都完成了任务\n",
    "        if all(self.completed):\n",
    "            self.done = True\n",
    "            # 基于最大完成时间给予最终奖励\n",
    "            max_distance = np.max(self.total_distances)\n",
    "            # 最大完成时间越小，奖励越大\n",
    "            final_reward = 100.0 / (1.0 + max_distance)\n",
    "            rewards += final_reward\n",
    "        \n",
    "        # 时间惩罚（鼓励快速完成）\n",
    "        rewards -= 0.1\n",
    "        \n",
    "        # 增加步数\n",
    "        self.steps += 1\n",
    "        \n",
    "        # 如果超过最大步数，结束episode\n",
    "        if self.steps > self.n_cities * 3:\n",
    "            self.done = True\n",
    "            # 未完成的惩罚\n",
    "            for i, comp in enumerate(self.completed):\n",
    "                if not comp:\n",
    "                    rewards[i] -= 20.0\n",
    "        \n",
    "        return self._get_state(), rewards, self.done, self.total_distances\n",
    "\n",
    "# 多智能体DQN\n",
    "class MultiAgentDQN:\n",
    "    def __init__(self, state_dim, action_dim, n_agents, \n",
    "                 learning_rate=0.001, gamma=0.99, epsilon_start=1.0, \n",
    "                 epsilon_end=0.01, epsilon_decay=0.995, buffer_capacity=10000, \n",
    "                 batch_size=64, target_update=10):\n",
    "        \n",
    "        self.n_agents = n_agents\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        # 为每个智能体创建Q网络和目标网络\n",
    "        self.policy_nets = [DQN(state_dim, action_dim).to(device) for _ in range(n_agents)]\n",
    "        self.target_nets = [DQN(state_dim, action_dim).to(device) for _ in range(n_agents)]\n",
    "        \n",
    "        for i in range(n_agents):\n",
    "            self.target_nets[i].load_state_dict(self.policy_nets[i].state_dict())\n",
    "            self.target_nets[i].eval()\n",
    "        \n",
    "        # 优化器\n",
    "        self.optimizers = [optim.Adam(net.parameters(), lr=learning_rate) for net in self.policy_nets]\n",
    "        \n",
    "        # 经验回放缓冲区（共享）\n",
    "        self.memory = ReplayBuffer(buffer_capacity)\n",
    "        \n",
    "        self.steps_done = 0\n",
    "    \n",
    "    def select_action(self, state, valid_actions_mask=None):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        actions = []\n",
    "        for i in range(self.n_agents):\n",
    "            # 使用epsilon-贪婪策略\n",
    "            if np.random.random() < self.epsilon:\n",
    "                # 随机选择有效动作\n",
    "                if valid_actions_mask is not None:\n",
    "                    valid_actions = np.where(valid_actions_mask[i])[0]\n",
    "                    action = np.random.choice(valid_actions) if len(valid_actions) > 0 else self.action_dim - 1\n",
    "                else:\n",
    "                    action = np.random.randint(self.action_dim)\n",
    "            else:\n",
    "                # 选择Q值最大的动作\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.policy_nets[i](state_tensor).cpu().numpy().squeeze()\n",
    "                \n",
    "                # 屏蔽无效动作\n",
    "                if valid_actions_mask is not None:\n",
    "                    q_values[~valid_actions_mask[i]] = -np.inf\n",
    "                \n",
    "                action = np.argmax(q_values)\n",
    "            \n",
    "            actions.append(action)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # 转换数据为张量\n",
    "        state_batch = torch.FloatTensor(np.array(batch.state)).to(device)\n",
    "        action_batch = torch.LongTensor(np.array(batch.action)).to(device)\n",
    "        reward_batch = torch.FloatTensor(np.array(batch.reward)).to(device)\n",
    "        next_state_batch = torch.FloatTensor(np.array(batch.next_state)).to(device)\n",
    "        done_batch = torch.FloatTensor(np.array(batch.done)).to(device)\n",
    "        \n",
    "        # 为每个智能体优化\n",
    "        for agent_idx in range(self.n_agents):\n",
    "            # 计算当前Q值\n",
    "            state_action_values = self.policy_nets[agent_idx](state_batch).gather(1, action_batch[:, agent_idx].unsqueeze(1))\n",
    "            \n",
    "            # 计算下一个状态的最大Q值\n",
    "            next_state_values = self.target_nets[agent_idx](next_state_batch).max(1)[0].detach()\n",
    "            \n",
    "            # 计算期望Q值\n",
    "            expected_state_action_values = (next_state_values * self.gamma * (1 - done_batch)) + reward_batch[:, agent_idx]\n",
    "            \n",
    "            # 计算Huber损失\n",
    "            loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "            \n",
    "            # 优化模型\n",
    "            self.optimizers[agent_idx].zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            for param in self.policy_nets[agent_idx].parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "            self.optimizers[agent_idx].step()\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def update_target_networks(self):\n",
    "        for i in range(self.n_agents):\n",
    "            self.target_nets[i].load_state_dict(self.policy_nets[i].state_dict())\n",
    "    \n",
    "    def save_models(self, path_prefix):\n",
    "        for i in range(self.n_agents):\n",
    "            torch.save(self.policy_nets[i].state_dict(), f\"{path_prefix}_agent_{i}.pth\")\n",
    "    \n",
    "    def load_models(self, path_prefix):\n",
    "        for i in range(self.n_agents):\n",
    "            self.policy_nets[i].load_state_dict(torch.load(f\"{path_prefix}_agent_{i}.pth\"))\n",
    "            self.target_nets[i].load_state_dict(self.policy_nets[i].state_dict())\n",
    "\n",
    "# 训练函数\n",
    "def train_multi_agent_tsp(cities, n_agents, episodes=1000, batch_size=64, \n",
    "                          target_update_freq=10, save_freq=100):\n",
    "    # 创建环境\n",
    "    env = MultiAgentTSPEnv(cities, n_agents)\n",
    "    \n",
    "    # 状态和动作维度\n",
    "    state_dim = len(env._get_state())\n",
    "    action_dim = env.n_cities + 1  # 可以移动到任何城市或停留\n",
    "    \n",
    "    # 创建多智能体DQN\n",
    "    maddqn = MultiAgentDQN(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        n_agents=n_agents,\n",
    "        batch_size=batch_size,\n",
    "        target_update=target_update_freq\n",
    "    )\n",
    "    \n",
    "    # 记录训练统计信息\n",
    "    episode_rewards = []\n",
    "    episode_max_times = []\n",
    "    epsilon_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_rewards = np.zeros(n_agents)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # 获取有效动作掩码（只能移动到未访问的城市或停留）\n",
    "            valid_actions_mask = np.zeros((n_agents, action_dim), dtype=bool)\n",
    "            for i in range(n_agents):\n",
    "                # 停留总是有效的\n",
    "                valid_actions_mask[i, -1] = True\n",
    "                \n",
    "                # 如果智能体未完成，可以移动到未访问的城市\n",
    "                if not env.completed[i]:\n",
    "                    for city in range(env.n_cities):\n",
    "                        if not env.visited[city]:\n",
    "                            valid_actions_mask[i, city] = True\n",
    "            \n",
    "            # 选择动作\n",
    "            actions = maddqn.select_action(state, valid_actions_mask)\n",
    "            \n",
    "            # 执行动作\n",
    "            next_state, rewards, done, distances = env.step(actions)\n",
    "            \n",
    "            # 存储经验\n",
    "            maddqn.memory.push(state, actions, next_state, rewards, done)\n",
    "            \n",
    "            # 优化模型\n",
    "            maddqn.optimize_model()\n",
    "            \n",
    "            # 更新状态和奖励\n",
    "            state = next_state\n",
    "            total_rewards += rewards\n",
    "        \n",
    "        # 更新目标网络\n",
    "        if episode % target_update_freq == 0:\n",
    "            maddqn.update_target_networks()\n",
    "        \n",
    "        # 更新epsilon\n",
    "        maddqn.update_epsilon()\n",
    "        \n",
    "        # 记录统计信息\n",
    "        episode_rewards.append(np.sum(total_rewards))\n",
    "        episode_max_times.append(np.max(distances))\n",
    "        epsilon_history.append(maddqn.epsilon)\n",
    "        \n",
    "        # 保存模型\n",
    "        if episode % save_freq == 0:\n",
    "            maddqn.save_models(f\"multi_agent_dqn_ep_{episode}\")\n",
    "        \n",
    "        # 打印进度\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}/{episodes} | Total Reward: {np.sum(total_rewards):.2f} | \"\n",
    "                  f\"Max Time: {np.max(distances):.2f} | Epsilon: {maddqn.epsilon:.4f}\")\n",
    "    \n",
    "    # 返回训练结果\n",
    "    return {\n",
    "        \"rewards\": episode_rewards,\n",
    "        \"max_times\": episode_max_times,\n",
    "        \"epsilon\": epsilon_history,\n",
    "        \"model\": maddqn,\n",
    "        \"env\": env\n",
    "    }\n",
    "\n",
    "# 评估函数\n",
    "def evaluate_multi_agent(model, env):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    paths = [[] for _ in range(model.n_agents)]\n",
    "    positions = [[] for _ in range(model.n_agents)]\n",
    "    total_distances = np.zeros(model.n_agents)\n",
    "    \n",
    "    while not done:\n",
    "        # 获取有效动作掩码\n",
    "        valid_actions_mask = np.zeros((model.n_agents, model.action_dim), dtype=bool)\n",
    "        for i in range(model.n_agents):\n",
    "            valid_actions_mask[i, -1] = True  # 停留总是有效的\n",
    "            if not env.completed[i]:\n",
    "                for city in range(env.n_cities):\n",
    "                    if not env.visited[city]:\n",
    "                        valid_actions_mask[i, city] = True\n",
    "        \n",
    "        # 选择动作（贪婪策略）\n",
    "        actions = model.select_action(state, valid_actions_mask)\n",
    "        \n",
    "        # 执行动作\n",
    "        next_state, rewards, done, distances = env.step(actions)\n",
    "        \n",
    "        # 记录路径和位置\n",
    "        for i in range(model.n_agents):\n",
    "            paths[i] = env.paths[i].copy()\n",
    "            positions[i].append(env.agent_positions[i])\n",
    "            total_distances[i] = env.total_distances[i]\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    # 计算最大完成时间\n",
    "    max_time = np.max(total_distances)\n",
    "    \n",
    "    return {\n",
    "        \"paths\": paths,\n",
    "        \"positions\": positions,\n",
    "        \"distances\": total_distances,\n",
    "        \"max_time\": max_time,\n",
    "        \"visited\": env.visited.copy()\n",
    "    }\n",
    "\n",
    "# 可视化函数\n",
    "def visualize_multi_agent_solution(cities, paths, distances, max_time):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # 绘制城市\n",
    "    plt.scatter(cities[:, 0], cities[:, 1], c='red', s=100, zorder=5)\n",
    "    for i, city in enumerate(cities):\n",
    "        plt.text(city[0], city[1], f\"{i}\", fontsize=12, ha='center', va='center', zorder=10)\n",
    "    \n",
    "    # 定义不同智能体的颜色\n",
    "    colors = ['blue', 'green', 'purple', 'orange', 'cyan', 'magenta']\n",
    "    \n",
    "    # 绘制路径\n",
    "    for i, path in enumerate(paths):\n",
    "        color = colors[i % len(colors)]\n",
    "        path_cities = np.array([cities[p] for p in path])\n",
    "        \n",
    "        # 绘制路径线\n",
    "        plt.plot(path_cities[:, 0], path_cities[:, 1], color=color, linewidth=1.5, alpha=0.7)\n",
    "        \n",
    "        # 绘制路径点\n",
    "        plt.scatter(path_cities[:, 0], path_cities[:, 1], color=color, s=50, alpha=0.7)\n",
    "        \n",
    "        # 添加箭头\n",
    "        for j in range(len(path) - 1):\n",
    "            start = path_cities[j]\n",
    "            end = path_cities[j+1]\n",
    "            dx = end[0] - start[0]\n",
    "            dy = end[1] - start[1]\n",
    "            plt.arrow(start[0], start[1], dx*0.9, dy*0.9, \n",
    "                      head_width=0.8, head_length=1.0, \n",
    "                      fc=color, ec=color, alpha=0.7)\n",
    "    \n",
    "    # 添加标题和信息\n",
    "    plt.title(f\"Multi-Agent TSP Solution (Agents: {len(paths)}, Max Time: {max_time:.2f})\")\n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加图例\n",
    "    legend_handles = []\n",
    "    for i in range(len(paths)):\n",
    "        color = colors[i % len(colors)]\n",
    "        legend_handles.append(plt.Line2D([0], [0], color=color, lw=2, \n",
    "                                        label=f\"Agent {i} (Dist: {distances[i]:.2f})\"))\n",
    "    plt.legend(handles=legend_handles)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置参数\n",
    "    n_cities = 15\n",
    "    n_agents = 3\n",
    "    episodes = 500\n",
    "    \n",
    "    # 生成城市坐标\n",
    "    np.random.seed(42)\n",
    "    cities = np.random.rand(n_cities, 2) * 100\n",
    "    \n",
    "    # 训练多智能体DQN\n",
    "    print(f\"Training Multi-Agent DQN for TSP with {n_cities} cities and {n_agents} agents...\")\n",
    "    training_results = train_multi_agent_tsp(\n",
    "        cities=cities,\n",
    "        n_agents=n_agents,\n",
    "        episodes=episodes,\n",
    "        batch_size=128,\n",
    "        target_update_freq=20\n",
    "    )\n",
    "    \n",
    "    # 获取训练后的模型和环境\n",
    "    maddqn = training_results[\"model\"]\n",
    "    env = training_results[\"env\"]\n",
    "    \n",
    "    # 评估模型\n",
    "    print(\"\\nEvaluating trained model...\")\n",
    "    eval_results = evaluate_multi_agent(maddqn, env)\n",
    "    \n",
    "    # 输出结果\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Max Completion Time: {eval_results['max_time']:.2f}\")\n",
    "    for i, (path, dist) in enumerate(zip(eval_results['paths'], eval_results['distances'])):\n",
    "        print(f\"Agent {i} Path: {path} | Distance: {dist:.2f}\")\n",
    "    \n",
    "    # 可视化解决方案\n",
    "    print(\"\\nVisualizing solution...\")\n",
    "    visualize_multi_agent_solution(\n",
    "        cities=cities,\n",
    "        paths=eval_results['paths'],\n",
    "        distances=eval_results['distances'],\n",
    "        max_time=eval_results['max_time']\n",
    "    )\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(training_results[\"rewards\"])\n",
    "    plt.title(\"Episode Rewards\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(training_results[\"max_times\"])\n",
    "    plt.title(\"Max Completion Time\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Time\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
